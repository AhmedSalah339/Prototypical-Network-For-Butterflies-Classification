{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-n_jxtPg8vHw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import argparse\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\CVprojects\\Butterflies\n"
     ]
    }
   ],
   "source": [
    "cd ..\\.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Sp-0DYPBdObg"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cs5zOAtqfObz"
   },
   "outputs": [],
   "source": [
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di\n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Qc_6QHkLdq_B"
   },
   "outputs": [],
   "source": [
    "from src.Prototypical_Networks_for_Few_shot_Learning_PyTorch.src.prototypical_batch_sampler import PrototypicalBatchSampler\n",
    "from src.Prototypical_Networks_for_Few_shot_Learning_PyTorch.src.prototypical_loss import prototypical_loss as loss_fn\n",
    "from src.Prototypical_Networks_for_Few_shot_Learning_PyTorch.src.protonet import ProtoNet\n",
    "from src.Prototypical_Networks_for_Few_shot_Learning_PyTorch.src.parser_util import get_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\CVprojects\\Butterflies\\src\n"
     ]
    }
   ],
   "source": [
    "cd src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wakXeTm7DsXQ"
   },
   "outputs": [],
   "source": [
    "from Butterfly200DataSet import Butterfly200DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\CVprojects\\Butterflies\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "SO3lhVl4WG_s",
    "outputId": "0ed36c04-9a5f-45df-ff64-6bd5068c301a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\CVprojects\\\\Butterflies\\\\configs\\\\splits\\\\split_dict2.pkl'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dict_name = os.path.join('configs\\\\splits',\"split_dict2.pkl\")\n",
    "split_dict_path = os.path.join(os.getcwd(),split_dict_name)\n",
    "split_dict_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-fSY3satgbSz"
   },
   "outputs": [],
   "source": [
    "def init_seed(opt):\n",
    "    torch.cuda.cudnn_enabled = True\n",
    "    np.random.seed(opt.manual_seed)\n",
    "    torch.manual_seed(opt.manual_seed)\n",
    "    torch.cuda.manual_seed(opt.manual_seed)\n",
    "def init_dataset(opt, mode):\n",
    "    dataset = Butterfly200DataSet(split_dict_path,mode=mode)\n",
    "    n_classes = len(np.unique(dataset.y))\n",
    "    if n_classes < opt.classes_per_it_tr or n_classes < opt.classes_per_it_val:\n",
    "        raise(Exception('There are not enough classes in the dataset in order ' +\n",
    "                        'to satisfy the chosen classes_per_it. Decrease the ' +\n",
    "                        'classes_per_it_{tr/val} option and try again.'))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def init_sampler(opt, labels, mode):\n",
    "    if 'train' in mode:\n",
    "        classes_per_it = opt.classes_per_it_tr\n",
    "        num_samples = opt.num_support_tr + opt.num_query_tr\n",
    "    else:\n",
    "        classes_per_it = opt.classes_per_it_val\n",
    "        num_samples = opt.num_support_val + opt.num_query_val\n",
    "\n",
    "    return PrototypicalBatchSampler(labels=labels,\n",
    "                                    classes_per_it=classes_per_it,\n",
    "                                    num_samples=num_samples,\n",
    "                                    iterations=opt.iterations)\n",
    "def init_dataloader(opt, mode):\n",
    "    dataset = init_dataset(opt, mode)\n",
    "    sampler = init_sampler(opt, dataset.y, mode)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_sampler=sampler)\n",
    "    return dataloader\n",
    "def init_protonet(opt):\n",
    "    '''\n",
    "    Initialize the ProtoNet\n",
    "    '''\n",
    "    device = 'cuda:0' if torch.cuda.is_available() and opt.cuda else 'cpu'\n",
    "    model = ProtoNet(x_dim=128).to(device)\n",
    "    return model\n",
    "def init_optim(opt, model):\n",
    "    '''\n",
    "    Initialize optimizer\n",
    "    '''\n",
    "    return torch.optim.Adam(params=model.parameters(),\n",
    "                            lr=opt.learning_rate)\n",
    "def init_lr_scheduler(opt, optim):\n",
    "    '''\n",
    "    Initialize the learning rate scheduler\n",
    "    '''\n",
    "    return torch.optim.lr_scheduler.StepLR(optimizer=optim,\n",
    "                                           gamma=opt.lr_scheduler_gamma,\n",
    "                                           step_size=opt.lr_scheduler_step)\n",
    "def save_list_to_file(path, thelist):\n",
    "    with open(path, 'w') as f:\n",
    "        for item in thelist:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aauYn7gvfeul"
   },
   "outputs": [],
   "source": [
    "def train(opt, tr_dataloader, model, optim, lr_scheduler, val_dataloader=None):\n",
    "    '''\n",
    "    Train the model with the prototypical learning algorithm\n",
    "    '''\n",
    "\n",
    "    device = 'cuda:0' if torch.cuda.is_available() and opt.cuda else 'cpu'\n",
    "\n",
    "    if val_dataloader is None:\n",
    "        best_state = None\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    best_acc = 0\n",
    "\n",
    "    best_model_path = os.path.join(opt.experiment_root, 'best_model.pth')\n",
    "    last_model_path = os.path.join(opt.experiment_root, 'last_model.pth')\n",
    "    for epoch in range(opt.epochs):\n",
    "        print('=== Epoch: {} ==='.format(epoch))\n",
    "        tr_iter = iter(tr_dataloader)\n",
    "        model.train()\n",
    "        \n",
    "        for batch in tqdm(tr_iter):\n",
    "            optim.zero_grad()\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "#             emb_model(x)\n",
    "#             model_output = model(activation['layer2'])\n",
    "            model_output = model(x)\n",
    "            loss, acc = loss_fn(model_output, target=y,\n",
    "                                n_support=opt.num_support_tr)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            train_loss.append(loss.item())\n",
    "            train_acc.append(acc.item())\n",
    "            torch.cuda.empty_cache()\n",
    "        avg_loss = np.mean(train_loss[-opt.iterations:])\n",
    "        avg_acc = np.mean(train_acc[-opt.iterations:])\n",
    "        print('Avg Train Loss: {}, Avg Train Acc: {}'.format(avg_loss, avg_acc))\n",
    "        lr_scheduler.step()\n",
    "        if val_dataloader is None:\n",
    "            continue\n",
    "        val_iter = iter(val_dataloader)\n",
    "        model.eval()\n",
    "        for batch in tqdm(val_iter):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "#             emb_model(x)\n",
    "#             model_output = model(activation['layer2'])\n",
    "            model_output = model(x)\n",
    "            loss, acc = loss_fn(model_output, target=y,\n",
    "                                n_support=opt.num_support_val)\n",
    "            val_loss.append(loss.item())\n",
    "            val_acc.append(acc.item())\n",
    "            torch.cuda.empty_cache()\n",
    "        avg_loss = np.mean(val_loss[-opt.iterations:])\n",
    "        avg_acc = np.mean(val_acc[-opt.iterations:])\n",
    "        postfix = ' (Best)' if avg_acc >= best_acc else ' (Best: {})'.format(\n",
    "            best_acc)\n",
    "        print('Avg Val Loss: {}, Avg Val Acc: {}{}'.format(\n",
    "            avg_loss, avg_acc, postfix))\n",
    "        if avg_acc >= best_acc:\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            best_acc = avg_acc\n",
    "            best_state = model.state_dict()\n",
    "\n",
    "    torch.save(model.state_dict(), last_model_path)\n",
    "\n",
    "    for name in ['train_loss', 'train_acc', 'val_loss', 'val_acc']:\n",
    "        save_list_to_file(os.path.join(opt.experiment_root,\n",
    "                                       name + '.txt'), locals()[name])\n",
    "\n",
    "    return best_state, best_acc, train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "\n",
    "def test(opt, test_dataloader, model):\n",
    "    '''\n",
    "    Test the model trained with the prototypical learning algorithm\n",
    "    '''\n",
    "    device = 'cuda:0' if torch.cuda.is_available() and opt.cuda else 'cpu'\n",
    "    avg_acc = list()\n",
    "    for epoch in tqdm(range(opt.test_epochs)):\n",
    "        test_iter = iter(test_dataloader)\n",
    "        for batch in test_iter:\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            model_output = model(x)\n",
    "            _, acc = loss_fn(model_output, target=y,\n",
    "                             n_support=opt.num_support_val)\n",
    "            avg_acc.append(acc.item())\n",
    "    avg_acc = np.mean(avg_acc)\n",
    "    print('Test Acc: {}'.format(avg_acc))\n",
    "\n",
    "    return avg_acc\n",
    "\n",
    "\n",
    "def eval(opt):\n",
    "    '''\n",
    "    Initialize everything and train\n",
    "    '''\n",
    "    options = get_parser().parse_args()\n",
    "\n",
    "    if torch.cuda.is_available() and not options.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "    init_seed(options)\n",
    "    test_dataloader = init_dataset(options)[-1]\n",
    "    model = init_protonet(options)\n",
    "    model_path = os.path.join(opt.experiment_root, 'best_model.pth')\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    test(opt=options,\n",
    "         test_dataloader=test_dataloader,\n",
    "         model=model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "q69hMyX_hgvC"
   },
   "outputs": [],
   "source": [
    "d = {\n",
    "    'experiment_root':'base_exp',\n",
    "     'epochs':200,\n",
    "     'test_epochs':20,\n",
    "    'iterations':200,\n",
    "     'cuda':True,\n",
    "     'classes_per_it_tr':5,\n",
    "    'lr_scheduler_step':20,\n",
    "    'num_query_tr':2,\n",
    "    'num_support_tr':1,\n",
    "         'num_query_val':1,\n",
    "     'num_support_val':1,\n",
    "    'learning_rate':0.0001,\n",
    "#      'check_point_path':'base_exp/best_after_blur_t48_val50.pth'\n",
    "     }\n",
    "options = get_parser().parse_args('')\n",
    "\n",
    "for key, value in d.items():\n",
    "        setattr(options, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\CVprojects\\\\Butterflies\\\\configs\\\\splits\\\\split_dict2.pkl'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dict_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 171.92it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 46/46 [00:00<00:00, 238.33it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Initialize everything and train\n",
    "'''\n",
    "# options = get_parser().parse_args()\n",
    "if not os.path.exists(options.experiment_root):\n",
    "    os.makedirs(options.experiment_root)\n",
    "\n",
    "if torch.cuda.is_available() and not options.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "init_seed(options)\n",
    "\n",
    "tr_dataloader = init_dataloader(options, 'train')\n",
    "val_dataloader = init_dataloader(options, 'val')\n",
    "# test_dataloader = init_dataloader(options, 'test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.EncoderProtoNet import EncoderProtoNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderProtoNet(proto_x_dim=128,proto_hid_dim=64,proto_z_dim=32)\n",
    "# model.load_state_dict(torch.load('E:\\\\CVprojects\\\\Butterflies\\\\src\\\\base_exp\\\\best_model_tr88_val87.pth'))\n",
    "\n",
    "\n",
    "enc_weights_path = 'base_exp\\\\best_model_embed_res.pth'\n",
    "model.load_encoder_weights(enc_weights_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.requires_gradient = True\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderProtoNet(\n",
       "  (encoder): SubResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proto): ProtoNet(\n",
       "    (encoder): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = init_optim(options, model)\n",
    "lr_scheduler = init_lr_scheduler(options, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iVOF6u9Fgqzb",
    "outputId": "04ef0830-a864-47d3-e981-58f62d9b6e16",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch: 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:48<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Train Loss: 395.06485773086547, Avg Train Acc: 0.31100000616163015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:16<00:00, 12.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Val Loss: 446.25671159267426, Avg Val Acc: 0.38600000940263274 (Best)\n",
      "=== Epoch: 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:49<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Train Loss: 179.89413833618164, Avg Train Acc: 0.39200000688433645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:17<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Val Loss: 282.2200828320533, Avg Val Acc: 0.4540000119060278 (Best)\n",
      "=== Epoch: 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:51<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Train Loss: 111.10251488685608, Avg Train Acc: 0.4815000064298511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:17<00:00, 11.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Val Loss: 232.80025629043578, Avg Val Acc: 0.49400001257658005 (Best)\n",
      "=== Epoch: 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:57<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Train Loss: 79.55961943149566, Avg Train Acc: 0.5600000068545341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:20<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Val Loss: 205.0301984171197, Avg Val Acc: 0.4780000109225512 (Best: 0.49400001257658005)\n",
      "=== Epoch: 4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [01:01<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Train Loss: 57.121012983322146, Avg Train Acc: 0.6150000025704503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:19<00:00, 10.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Val Loss: 179.5149845993519, Avg Val Acc: 0.48700001172721386 (Best: 0.49400001257658005)\n",
      "=== Epoch: 5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [01:01<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Train Loss: 39.74523802812398, Avg Train Acc: 0.7030000014603138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:23<00:00,  8.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Val Loss: 160.01845502042744, Avg Val Acc: 0.4820000118762255 (Best: 0.49400001257658005)\n",
      "=== Epoch: 6 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [01:02<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Train Loss: 33.99303068153468, Avg Train Acc: 0.7250000002980233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:20<00:00,  9.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Val Loss: 177.0726792317629, Avg Val Acc: 0.45900000996887685 (Best: 0.49400001257658005)\n",
      "=== Epoch: 7 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [01:00<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Train Loss: 25.949398875782776, Avg Train Acc: 0.7699999976158142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [00:19<00:00, 10.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Val Loss: 172.43178587335348, Avg Val Acc: 0.47400001049041746 (Best: 0.49400001257658005)\n",
      "=== Epoch: 8 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████▊                                                                 | 39/200 [00:11<00:46,  3.49it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_104516\\756395412.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0moptim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             lr_scheduler=lr_scheduler)\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mbest_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_104516\\369111801.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(opt, tr_dataloader, model, optim, lr_scheduler, val_dataloader)\u001b[0m\n\u001b[0;32m     30\u001b[0m             loss, acc = loss_fn(model_output, target=y,\n\u001b[0;32m     31\u001b[0m                                 n_support=opt.num_support_tr)\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = train(opt=options,\n",
    "            tr_dataloader=tr_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            model=model,\n",
    "            optim=optim,\n",
    "            lr_scheduler=lr_scheduler)\n",
    "best_state, best_acc, train_loss, train_acc, val_loss, val_acc = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('base_exp/best_model_95val_82tr.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.requires_gradient = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6ehYKK1y92P",
    "outputId": "6e7149f1-824e-46d0-ea76-5fa19c5d4754"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4168/4168 [01:30<00:00, 46.24it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = init_dataloader(options, 'test')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c88cHxT-cS1B",
    "outputId": "70a13655-3eff-402e-f7f3-39d96b1acc71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with last model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [02:07<00:00,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.9118999935984612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9118999935984612"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Testing with last model..')\n",
    "test(opt=options,\n",
    "      test_dataloader=test_dataloader,\n",
    "      model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4es1kBAwlfZl"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(best_state)\n",
    "print('Testing with best model..')\n",
    "test(opt=options,\n",
    "      test_dataloader=test_dataloader,\n",
    "      model=model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BaseExp.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
