{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import cv2\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_protonet():\n",
    "    '''\n",
    "    Initialize the ProtoNet\n",
    "    '''\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    model = ProtoNet().to(device)\n",
    "    return model\n",
    "def euclidean_dist(x, y):\n",
    "    '''\n",
    "    Compute euclidean distance between two tensors\n",
    "    '''\n",
    "    # x: N x D\n",
    "    # y: M x D\n",
    "\n",
    "    return torch.pow(x-y,2).sum().detach().cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes_paths(paths):\n",
    "    classes_paths = {}\n",
    "    for path in paths:\n",
    "        cls = int(os.path.basename(path.split('.')[0]))-1\n",
    "        if cls not in classes_paths:\n",
    "            classes_paths[cls] = []\n",
    "        classes_paths[cls].append(path)\n",
    "    return classes_paths\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di\n",
    "def embed_images(paths,model):\n",
    "    images_tensors = []\n",
    "    for path in paths:\n",
    "        x = torch.load(path)\n",
    "        images_tensors.append(x)\n",
    "    \n",
    "    images_tensors_tensor = torch.stack(images_tensors)\n",
    "    images_tensors_tensor = images_tensors_tensor.cuda()\n",
    "#     print(images_tensors_tensor.shape)\n",
    "    emb_vectors = model(images_tensors_tensor)\n",
    "    return emb_vectors.cpu().detach()\n",
    "def get_distances_embedding(emb_vectors,ref):\n",
    "    vec1 = ref[0]\n",
    "    vec2 = ref[1]\n",
    "    ds1 = []\n",
    "    ds2 = []\n",
    "    for vec in emb_vectors:\n",
    "        d1 = euclidean_dist(vec,vec1)\n",
    "        d2 = euclidean_dist(vec,vec2)\n",
    "        ds1.append(d1)\n",
    "        ds2.append(d2)\n",
    "    return np.array(ds1),np.array(ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_image_display(files_paths):\n",
    "    rand_idx = np.random.randint(len(files_paths)-1)\n",
    "    img = cv2.imread(files_paths[rand_idx])\n",
    "    print(img.shape)\n",
    "    plt.imshow(img)\n",
    "def convert_paths_to_jpg(paths):\n",
    "    return [os.path.splitext(p)[0]+'.jpg' for p in paths]\n",
    "def convert_paths_to_jpg(paths):\n",
    "    return [os.path.splitext(p)[0]+'.jpg' for p in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(filepath,expression='*.json'):\n",
    "    '''\n",
    "    Walks over a directory and its children to get all children json files pathes\n",
    "    Arguments:\n",
    "    file_path: string that specifies the path to the data parent directory \n",
    "    Returns:\n",
    "    all_files: List of all the filepaths of the matching expression files included in the directory\n",
    "    '''\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        files = glob.glob(os.path.join(root,expression))\n",
    "        for f in files :\n",
    "            all_files.append(os.path.abspath(f))\n",
    "    return all_files\n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\CVprojects\\Butterflies\n"
     ]
    }
   ],
   "source": [
    "cd ..\\..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.EncoderProtoNet import EncoderProtoNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderProtoNet(\n",
       "  (encoder): SubResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proto): ProtoNet(\n",
       "    (encoder): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EncoderProtoNet(proto_x_dim=128)\n",
    "weights_path = os.path.join('checkpoints','best_model_95val_82tr.pth')\n",
    "model.load_state_dict(torch.load(weights_path))\n",
    "model = model.to('cuda')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_paths = get_files('Data','*sub.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25279"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_to_paths = get_classes_paths(files_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 200/200 [11:03<00:00,  3.32s/it]\n"
     ]
    }
   ],
   "source": [
    "max_clust_num = 3\n",
    "clusters_paths = {i:[] for i in range(max_clust_num)}\n",
    "torch.backends.cudnn.enabled = False\n",
    "for i in  tqdm(range(200)):\n",
    "    paths = classes_to_paths[i][:100]\n",
    "    embs = embed_images(paths,model)\n",
    "    torch.cuda.empty_cache()\n",
    "    kmeans = KMeans(n_clusters=max_clust_num, random_state=0).fit(embs)\n",
    "    for i,m in enumerate(kmeans.labels_):\n",
    "        clusters_paths[m].append(paths[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = []\n",
    "for j in range(max_clust_num):\n",
    "    ks.append(get_classes_paths(clusters_paths[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_images_per_class = 100\n",
    "new_ks = ks\n",
    "np.random.seed(0)\n",
    "for i in range(200):\n",
    "    ls = []\n",
    "    cur_class_clust_paths = {}\n",
    "    cur_max_images_per_class = max_images_per_class\n",
    "    cur_max_clust_num = max_clust_num\n",
    "    \n",
    "    # get the lengths of the clusters examples\n",
    "    for j in range(max_clust_num):\n",
    "        cur_class_clust_paths[j] = ks[j][i]\n",
    "        ls.append(len(ks[j][i]))\n",
    "    # loop on the clusters from the least samples number to the most    \n",
    "    indeces = np.argsort(ls)\n",
    "    for idx in indeces:\n",
    "        cur_required_samples_per_clust = int(np.floor(cur_max_images_per_class/cur_max_clust_num))\n",
    "        num_samples = min(cur_required_samples_per_clust,ls[idx])\n",
    "        new_ks[idx][i] = ks[idx][i][:num_samples]\n",
    "        \n",
    "        cur_max_images_per_class = cur_max_images_per_class-num_samples\n",
    "        cur_max_clust_num = cur_max_clust_num-1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_rat = 0.25\n",
    "test_rat = 0.25\n",
    "max_samples_per_class = 90\n",
    "train_paths = []\n",
    "val_paths = []\n",
    "test_paths = []\n",
    "slow_test_paths = []\n",
    "for i in range(max_clust_num):\n",
    "    classes_samples = new_ks[i]\n",
    "    for key in classes_samples:\n",
    "        cur_paths = np.array(classes_samples[key])\n",
    "        l = len(cur_paths)\n",
    "        l2 = min(l,max_samples_per_class)\n",
    "        num_val = int(l2*val_rat)\n",
    "        num_test = int(l2*val_rat)\n",
    "\n",
    "        all_indeces = np.arange(l)\n",
    "        np.random.shuffle(all_indeces)\n",
    "\n",
    "        if len(all_indeces)>max_samples_per_class:\n",
    "            indeces_slow_test = all_indeces[max_samples_per_class:]\n",
    "            all_indeces_down = all_indeces[:max_samples_per_class]\n",
    "        else:\n",
    "            indeces_slow_test = []\n",
    "            all_indeces_down = all_indeces\n",
    "\n",
    "        val_indeces = all_indeces_down[:num_val]\n",
    "        test_indeces = all_indeces_down[num_val:num_val+num_test]\n",
    "        train_indeces = all_indeces_down[num_val+num_test:]\n",
    "\n",
    "        train_paths.extend(cur_paths[train_indeces])\n",
    "        val_paths.extend(cur_paths[val_indeces])\n",
    "        test_paths.extend(cur_paths[test_indeces])\n",
    "        slow_test_paths.extend(cur_paths[test_indeces])\n",
    "        slow_test_paths.extend(cur_paths[indeces_slow_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'E:\\\\CVprojects\\\\Butterflies\\Data\\\\images_small\\\\'\n",
    "designed_splits = {\n",
    "    'train':{\n",
    "    base+'110.Chitoria_ulupi':[1,2,3,4,7,10,13,17,20,27,30,31,36],\n",
    "        base+'113.Sasakia_charonda':[1,4,7,10,13,16,19,20,22,25,28,31,33,34,37,39,42,44,48,53,61,63,68,70,72,76,79,81],\n",
    "        base+'133.Damora_sagana':[2,4,1,10,16,17,19,21,24,27,30,33,38,42,46,49,50,52,54,55],\n",
    "        base+'006.Graphium_agamemnon':[1,2,5,8,9,10,12,18,22,26,28,31,34,37,40,42,43,45,49,55,58,65],\n",
    "        base+'175.Rapala_nissa':[1,2,3,4,9,10,11,12,14,17,19,23,24,27,32,36,38,39,40,41,42,45,50,52,54,57,64,65,66,67],\n",
    "        base+'014.Meandrusa_sciron':[1,2,3,5,6,10,11,12,13,14,15,18,19,21,24,25,26,29,34,35,36,39,43],\n",
    "        base+'118.Euthalia_niepelti':[1,2,3,5,6,8,11,13,16,18,22,24,27,28,29,30,32],\n",
    "        base+'142.Doleschallia_bisaltide':[1,4,6,8,9,10,11,15,17,19,21,24,26,29,31,33,35,45,47,48,51,54,57],\n",
    "        base+'155.Libythea_myrrha':[1,2,5,7,10,11,14,15,18,19,20,21,25,27,31,32,37,39,40,42,46,49,50,53,58,59,62,63,66,69,73,76,79,80],\n",
    "        base+'174.Mahathala_ameria':[1,2,3,4,5,6,12,14,19,23,24,25,29,31,32,34,37,38,40,43,44,45,46,47],\n",
    "        base+'179.Zizeeria_maha':[1,2,3,4,6,7,11,14,17,18,21,25,26,28,29,30,34,37,38,39,46,51,52,53,54,59,60,61,64,66,69,72,73,74,75,76,78,79,80,84],\n",
    "        base+'166.Arhopala_rama':[1,2,4,6,7,10,11,15,16,18,22,25,26,27,28,29,32,36,37,38,41,44]\n",
    "        \n",
    "    },\n",
    "    'val':{\n",
    "    base+'110.Chitoria_ulupi':[5,8,11,12,18,21,25,28,32],\n",
    "        base+'113.Sasakia_charonda':[2,5,8,11,14,17,21,23,26,29,32,35,38,40,43,46,49,55,56,59,62,64,73,75,80,83,84],\n",
    "        base+'133.Damora_sagana':[3,5,6,11,18,20,22,28,32,34,37,39,41,34,44,51,57,58,59],\n",
    "        base+'006.Graphium_agamemnon':[3,6,11,14,16,19,21,25,29,33,35,38,41,44,46,52,53,54,59,61],\n",
    "        base+'175.Rapala_nissa':[5,7,13,15,22,25,28,30,31,34,37,46,47,51,53,55,60,68,70],\n",
    "        base+'014.Meandrusa_sciron':[7,16,20,22,27,33,37,42,44,45],\n",
    "        base+'118.Euthalia_niepelti':[4,7,9,10,14,19,23,25,31],\n",
    "        base+'142.Doleschallia_bisaltide':[2,5,14,18,20,25,27,30,34,37,39,41,43,46,52,55,58,60],\n",
    "        base+'155.Libythea_myrrha':[3,6,9,12,16,22,26,30,33,36,38,44,47,48,52,55,70,71,72,74,77,81],\n",
    "        base+'174.Mahathala_ameria':[7,8,10,20,22,26,28,33,35,41,48,50,],\n",
    "        base+'179.Zizeeria_maha':[5,9,10,15,16,19,22,27,33,35,41,43,47,50,55,57,62,67,77,81,83],\n",
    "        base+'166.Arhopala_rama':[3,12,17,1,21,24,30,33,34,39,42,45]\n",
    "    },\n",
    "    'test':{\n",
    "    base+'110.Chitoria_ulupi':[6,9,15,19,22,23,24,29,35],\n",
    "        base+'113.Sasakia_charonda':[3,6,9,12,15,18,24,27,30,36,41,47,50,51,54,57,60,65,69,71,74,77,82,85],\n",
    "        base+'133.Damora_sagana':[7,12,14,23,25,29,31,35,36,40,45,48,53,56],\n",
    "        base+'006.Graphium_agamemnon':[4,7,13,15,17,20,23,24,30,32,36,39,47,48,50,51,56,62,63,64,70],\n",
    "        base+'175.Rapala_nissa':[6,8,16,18,20,26,29,33,35,43,44,49,56,58,59,61,62,69,71],\n",
    "        base+'014.Meandrusa_sciron':[9,17,23,28,30,32,40,46,47,48],\n",
    "        base+'118.Euthalia_niepelti':[12,15,17,20,21,26],\n",
    "        base+'142.Doleschallia_bisaltide':[3,7,16,22,23,28,32,36,38,40,42,44,49,50,53,65,59,61],\n",
    "        base+'155.Libythea_myrrha':[4,8,13,17,23,28,34,35,41,43,45,51,60,61,64,65,67,68,69,75,78],\n",
    "        base+'174.Mahathala_ameria':[9,11,13,21,27,30,36,42,49,51,54,56,57],\n",
    "        base+'179.Zizeeria_maha':[8,12,13,20,23,24,31,32,36,42,44,45,48,49,56,58,63,65,68,71,82],\n",
    "        base+'166.Arhopala_rama':[13,14,19,20,23,31,35,40,43]\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'122'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_number(num,suffix = ''):\n",
    "    s = str(num)\n",
    "    \n",
    "    l = len(s)\n",
    "    while l <3:\n",
    "        s = '0'+s\n",
    "        l+=1\n",
    "    return s+suffix\n",
    "\n",
    "format_number(122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths_designed = [os.path.join(path,format_number(num,'sub.pt'))for path in designed_splits['train'] for num in designed_splits['train'][path]]\n",
    "val_paths_designed = [os.path.join(path,format_number(num,'sub.pt'))for path in designed_splits['val'] for num in designed_splits['train'][path]]\n",
    "test_paths_designed = [os.path.join(path,format_number(num,'sub.pt'))for path in designed_splits['test'] for num in designed_splits['train'][path]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(path):\n",
    "    return int(os.path.basename(path.split('.')[0]))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[109, 112, 132, 5, 174, 13, 117, 141, 154, 173, 178, 165]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "design_classes = [get_class(key)for key in designed_splits['train']]\n",
    "design_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = [p for p in train_paths if get_class(p) not in design_classes]\n",
    "val_paths = [p for p in val_paths if get_class(p) not in design_classes]\n",
    "test_paths = [p for p in test_paths if get_class(p) not in design_classes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths+=train_paths_designed\n",
    "val_paths+=val_paths_designed\n",
    "test_paths+=test_paths_designed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = [os.path.relpath(p,'.') for p in train_paths ]\n",
    "val_paths = [os.path.relpath(p,'.') for p in val_paths ]\n",
    "test_paths = [os.path.relpath(p,'.') for p in test_paths ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data\\\\images_small\\\\001.Atrophaneura_horishanus\\\\015sub.pt'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dict = {\n",
    "    'train':train_paths,\n",
    "    'val':val_paths,\n",
    "    'test':test_paths,\n",
    "}\n",
    "delim = os.sep\n",
    "split_dict_name = 'configs'+delim+'splits'+delim+\"split_dict_hybrid_clust.pkl\"\n",
    "save_dict(split_dict,split_dict_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd ..\\.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delim = os.sep\n",
    "# d = load_dict('configs'+delim+'splits'+delim+\"split_dict_hybrid_clust.pkl\")\n",
    "# l = len('sub.pt')\n",
    "# len(d['train'])+len(d['val'])+len(d['test'])\n",
    "# for key in d:\n",
    "#     d[key] = [x[:-l]+'.jpg'for x in d[key]]\n",
    "# save_dict(d,'configs'+delim+'splits'+delim+\"split_dict_hybrid_clust.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
